{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHeJp5SjpO1K"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "th-jWnckpm0l"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eVFOESCrnFAQ",
    "outputId": "62476191-cc6e-43fa-8ead-01c445793e56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /kaggle/input/suicide-watch\n",
      "                                                text        class\n",
      "0  Ex Wife Threatening SuicideRecently I left my ...      suicide\n",
      "1  Am I weird I don't get affected by compliments...  non-suicide\n",
      "2  Finally 2020 is almost over... So I can never ...  non-suicide\n",
      "3          i need helpjust help me im crying so hard      suicide\n",
      "4  I’m so lostHello, my name is Adam (16) and I’v...      suicide\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 232074 entries, 0 to 232073\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    232074 non-null  object\n",
      " 1   class   232074 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.5+ MB\n",
      "None\n",
      "                                                     text    class\n",
      "count                                              232074   232074\n",
      "unique                                             232074        2\n",
      "top     I still haven't beaten the first boss in Hollo...  suicide\n",
      "freq                                                    1   116037\n",
      "(232074, 2)\n",
      "Index(['text', 'class'], dtype='object')\n",
      "                                                text  class\n",
      "0  Ex Wife Threatening SuicideRecently I left my ...      1\n",
      "1  Am I weird I don't get affected by compliments...      0\n",
      "2  Finally 2020 is almost over... So I can never ...      0\n",
      "3          i need helpjust help me im crying so hard      1\n",
      "4  I’m so lostHello, my name is Adam (16) and I’v...      1\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"nikhileswarkomati/suicide-watch\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# Find the CSV file within the downloaded directory\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        csv_file_path = os.path.join(path, filename)\n",
    "        break\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "df.dtypes\n",
    "\n",
    "# Convert 'class' column to numerical representation (0 or 1)\n",
    "df['class'] = df['class'].map({'suicide': 1, 'non-suicide': 0})\n",
    "\n",
    "# Display the updated DataFrame to verify the changes\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyI_O9espRZv"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9keQKPRypjmg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PMsh1FcUnk6u",
    "outputId": "951cdcf9-1af0-4f20-ff4c-3a0985fde99e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93635678121297\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     23287\n",
      "           1       0.94      0.93      0.94     23128\n",
      "\n",
      "    accuracy                           0.94     46415\n",
      "   macro avg       0.94      0.94      0.94     46415\n",
      "weighted avg       0.94      0.94      0.94     46415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data\n",
    "X = df['text']\n",
    "y = df['class']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer to convert text into numerical features\n",
    "vectorizer = TfidfVectorizer(max_features=5000) # Limit features for performance\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data using the same vectorizer\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_vec)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vkZe29_jng6X",
    "outputId": "e37d7cb0-97ab-4c22-e154-075944904e76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for 'I am feeling okay.': 0\n"
     ]
    }
   ],
   "source": [
    "# Example prediction for new text\n",
    "new_text = [\"I am feeling okay.\"]\n",
    "new_text_vec = vectorizer.transform(new_text)\n",
    "prediction = model.predict(new_text_vec)\n",
    "print(f\"Prediction for '{new_text[0]}': {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYseXGRPpJ9l"
   },
   "source": [
    "# RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtbcrOhcyyz4"
   },
   "source": [
    "## Installs and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4kA0RHXq3VM"
   },
   "source": [
    "### Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-38WtfTyuxC4",
    "outputId": "b54ec147-dd89-4741-8957-a8754004126b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fg32RClLtYAP"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePuDiEDPuylg"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n773bMjiq62S"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpvxL6iyu0yv"
   },
   "source": [
    "### Load Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420,
     "referenced_widgets": [
      "32d70b082dd944c79e3644a7a3f73be6",
      "018681ce72af443795a6005c026fee87",
      "a8af72d66737458386c7912d24917133",
      "8d7c4aff6a1b42d4ad11f083d1548689",
      "fb14ff58b7664cd3ad213baad2ba2b05",
      "4ab680c22d6e4039a9c61797b90585ec",
      "d66f9df3bf754b38ab04c2b7a0a60e7d",
      "03cbd38bfb844867ae4cd2ef1cc51966",
      "c24bdcd6a4394a1194137a6b5542a0b4",
      "64a68de64af14ac3b22c39bb3f87843b",
      "968008677a3440d3b334adf44dd0eccc",
      "5b29cb4106f64b5291e354f9aa661511",
      "a58114a356c44eedbd29ee9e9779e895",
      "334e058b4e9a438c886337871fcf7c92",
      "90cdf511200046e795e945c088284830",
      "fec53bf0683d4d4fa305bff5d0b38382",
      "1b32b21fe03c47fd82685826818e7ea6",
      "4a8b206bc1d9467383b9309cb10947c4",
      "1f09ca8c68b34d538ef080983ee87822",
      "50add0415a7d43f4a44d088f58833d25",
      "636539db46064f42991d09690459397b",
      "c40e2cd2db8b4e8699d97cb2523264da",
      "0fcc9f69261844209d8ce7ba39035a34",
      "fa9ad42b5dbe482aaa363d71d5bf37c3",
      "edb72189451c45668d19ad5b029ac4bf",
      "4e89ef20fd994c7385bb73946a21b3a5",
      "c5c7fcbd3abb4fcbbd621387dfb61471",
      "2d2d74f17d1d4498ac169f3c1de351e5",
      "f282fe955b7346a48557af80174efb86",
      "33c19815a4224477b5e3310a1a973f9d",
      "633ad896738e4e058aa91c51f344f6f3",
      "0b1d391a820c43439e5227f156478bd5",
      "6fe2e0b545bb43419d6390a272f50fdb",
      "f1196d14645247a8965762f1a162809b",
      "985cde06f9cd421d886d6e1191928b44",
      "f3501be3fcfb432e8690ff26b66a90a7",
      "8cfd4c0375b64d3cbdb163c8727dc4f8",
      "b40b0c90acac4afebdf6053a20284f7a",
      "8929356b3b0048e59639115ec10a536d",
      "552f85274dd847558448754a95d346f6",
      "861a3748fb234aed8bd16a11d4e2c15d",
      "aaf72f8035df488090f6951f1f4efd68",
      "e0ba68bb78684db7b3792b0808b1e51c",
      "fe572e1e818c4e25933b6cf0eeedf90b",
      "50e58747956e4674b8529c7e1d2bef81",
      "75120b83e9214d34b087c541ec1d8a39",
      "30c55ba20ddd42939324a25238075bef",
      "b2dd1393e6e6447088451a0a45325148",
      "a41b4401d1294d55b251bb2305489515",
      "2ce37be0bc9543f5aeb52a2611c9bb6e",
      "786582de6ca241779edf4a95579d042b",
      "fd97cc0d32d3440784a91573f8e36be4",
      "c6d875a634744625b0414e0ccc42e03d",
      "fb1eb9632be74d659d87748c97467206",
      "b1586c4d93904564b4c26f0ed3cb0dce",
      "f53fe60e624b4e659756a6c35c8de296",
      "07651f5e4f5245cd87e2b5b5e7566f2c",
      "5993981033864b709525e40bfbaf6587",
      "5e938ddb53154ef1b41c80d35915e3d9",
      "196c3ba6b07c45d68e5febb6659ba4d2",
      "d454f5ecfec64f13ab5f6f775f9b8026",
      "190b3acd4e884ae9965c39fad44ad5ca",
      "1e7865ada262458f87de19c50235fbb8",
      "42b252a7a94e40ab83a9015068bf0e62",
      "4bac57fb135a4f6d93164b59ca72fea1",
      "4657f4c9655f4cc5ac035e54ff5587be"
     ]
    },
    "id": "mWj8xtgEu9Uc",
    "outputId": "9a6b984c-6707-4210-8d78-347cfc5b38c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d70b082dd944c79e3644a7a3f73be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b29cb4106f64b5291e354f9aa661511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fcc9f69261844209d8ce7ba39035a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1196d14645247a8965762f1a162809b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e58747956e4674b8529c7e1d2bef81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53fe60e624b4e659756a6c35c8de296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = TFRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)  # 2 labels: suicide/non-suicide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBRw4VGPu-Rv"
   },
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cc2MYkCWvKmE"
   },
   "outputs": [],
   "source": [
    "# Calculate the number of samples for 35% of the data\n",
    "sample_size = int(0.35 * len(df))\n",
    "\n",
    "# Sample 35% of the data\n",
    "df_sample = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Prepare the data\n",
    "X = df_sample['text'].tolist()\n",
    "y = df_sample['class'].tolist()\n",
    "\n",
    "# Tokenize the text data\n",
    "encoded_data = tokenizer(X, padding=True, truncation=True, max_length=256, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWxDfRKbvXM-"
   },
   "outputs": [],
   "source": [
    "# Convert labels to TensorFlow tensors\n",
    "y = tf.convert_to_tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nI1lwyzBvNQU"
   },
   "source": [
    "Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nw1R4mS9x42o"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wOf4einxx7EP"
   },
   "outputs": [],
   "source": [
    "# Access the input_ids from the encoded data for splitting\n",
    "# Convert input_ids to NumPy array before splitting\n",
    "input_ids_np = encoded_data['input_ids'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZ8Ly5iY3Rl-"
   },
   "outputs": [],
   "source": [
    "# Convert TensorFlow tensors to NumPy arrays\n",
    "input_ids_np = encoded_data['input_ids'].numpy()\n",
    "y_np = y.numpy() if hasattr(y, \"numpy\") else y\n",
    "\n",
    "# Split the data\n",
    "X_train_encoded, X_test_encoded, y_train, y_test = train_test_split(\n",
    "    input_ids_np, y_np, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert back to tensors if needed (for TensorFlow models)\n",
    "import tensorflow as tf\n",
    "X_train_encoded = tf.convert_to_tensor(X_train_encoded)\n",
    "X_test_encoded = tf.convert_to_tensor(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kZOXG56x9O4"
   },
   "outputs": [],
   "source": [
    "# Convert the split data back to TensorFlow tensors\n",
    "X_train_encoded = tf.convert_to_tensor(X_train_encoded)\n",
    "X_test_encoded = tf.convert_to_tensor(X_test_encoded)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "y_test = tf.convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76LwzPC1vfk8"
   },
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQlGhfoMvseW"
   },
   "outputs": [],
   "source": [
    "# Define the optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zF32HDzyvuh9"
   },
   "source": [
    "### Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lX8g0x_09ydC",
    "outputId": "647e933c-b356-4872-ea7d-c46193905ebb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2708/2708 [==============================] - 1598s 573ms/step - loss: 0.0765 - accuracy: 0.9716 - val_loss: 0.0328 - val_accuracy: 0.9893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x78d197d22b50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    x=X_train_encoded,\n",
    "    y=y_train,\n",
    "    validation_data=(X_test_encoded, y_test),\n",
    "    epochs=1,  # Adjust the number of epochs as needed\n",
    "    batch_size=24 # Reduced batch size to 24\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gy2dxQYTv0ZT"
   },
   "source": [
    "### Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o5X-08U5v2T-",
    "outputId": "3f2a934b-4f7a-42cc-ecf7-90185ffed63c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "508/508 [==============================] - 123s 242ms/step - loss: 0.0328 - accuracy: 0.9893\n",
      "Loss: 0.03282339125871658\n",
      "Accuracy: 0.9892889857292175\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_encoded, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyrEUJfRwCX1"
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_C9_YTqwBpm",
    "outputId": "5282e39b-cc17-4cd4-89cc-0a1600023163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "Prediction for 'I am feeling okay.': 0\n"
     ]
    }
   ],
   "source": [
    "# Example prediction for new text\n",
    "new_text = [\"I am feeling okay.\"]\n",
    "new_text_encoded = tokenizer(new_text, padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
    "prediction = model.predict(new_text_encoded)\n",
    "predicted_class = tf.argmax(prediction.logits, axis=1).numpy()[0]\n",
    "print(f\"Prediction for '{new_text[0]}': {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xl_ugd0tlUoA"
   },
   "outputs": [],
   "source": [
    "# Export the model weights\n",
    "\n",
    "model.save_weights('/content/drive/MyDrive/ColabNoteBooks/Capstone/roberta_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4C0z2v9bkn7"
   },
   "outputs": [],
   "source": [
    "# export the model weights in a zip file\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def zip_directory(folder_path, zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                zipf.write(os.path.join(root, file),\n",
    "                           os.path.relpath(os.path.join(root, file),\n",
    "                                           os.path.join(folder_path, '..')))\n",
    "\n",
    "zip_directory('/content/drive/MyDrive/ColabNoteBooks/Capstone/roberta_weights', '/content/drive/MyDrive/ColabNoteBooks/Capstone/roberta_weights.zip')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
